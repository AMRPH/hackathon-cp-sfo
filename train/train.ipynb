{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from encoder_module import Encoder\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49775,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 10000\n",
    "y_unique = dataset['class'].unique()\n",
    "y_dict = {'Жилищное строительство': 0,\n",
    "          'Социальные объекты': 1,\n",
    "          'Транспортная инфраструктура': 2,\n",
    "          'Финансовые показатели': 3,\n",
    "          'Перспективы и планы на будущее': 4}\n",
    "\n",
    "y_text = dataset['class'] #.iloc[0:batch_size]\n",
    "y_np = np.array(y_text.apply(lambda x: y_dict[x]).to_list())\n",
    "y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_text = dataset['sentence'].iloc[0:5000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('1.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[5000:10000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('2.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[10000:15000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('3.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[15000:20000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('4.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[20000:25000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('5.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[25000:30000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('6.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[30000:35000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('7.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[35000:40000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('8.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[40000:45000]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('9.npy', X_np)\n",
    "\n",
    "# x_text = dataset['sentence'].iloc[45000:49776]\n",
    "# X_np = encoder.encode(x_text.to_list()).numpy()\n",
    "# np.save('10.npy', X_np)\n",
    "# \n",
    "# np1 = np.load('1.npy')\n",
    "# np2 = np.load('2.npy')\n",
    "# np3 = np.load('3.npy')\n",
    "# np4 = np.load('4.npy')\n",
    "# np5 = np.load('5.npy')\n",
    "# np6 = np.load('6.npy')\n",
    "# np7 = np.load('7.npy')\n",
    "# np8 = np.load('8.npy')\n",
    "# np9 = np.load('9.npy')\n",
    "# np10 = np.load('10.npy')\n",
    "\n",
    "# X_np = np.concatenate((np1, np2, np3, np4, np5, np6, np7, np8, np9, np10), axis=0)\n",
    "# X_np.shape\n",
    "# np.save('X_emb.npy', X_np)\n",
    "\n",
    "X_np = np.load('X_emb.npy')\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_np).type(torch.float)\n",
    "y = torch.from_numpy(y_np).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=42) #stratify=y\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=768, out_features=2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2048, out_features=2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2048, out_features=2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2048, out_features=2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2048, out_features=5)\n",
    ")\n",
    "classificator.to(device)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=classificator.parameters(), lr=0.05)\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_milticlass_classification_model(cl_model, n_epochs, print_every_epoch=100):\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        cl_model.train()\n",
    "\n",
    "        y_logits = cl_model(X_train)\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "        \n",
    "        loss = loss_fn(y_logits, y_train) \n",
    "        acc = accuracy_fn(y_true=y_train,\n",
    "                          y_pred=y_pred)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        cl_model.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_logits = cl_model(X_test)\n",
    "            test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "            \n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_test,test_pred)\n",
    "\n",
    "        if epoch % print_every_epoch == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") \n",
    "            \n",
    "learn_milticlass_classification_model(classificator, 2000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classificator.state_dict(), \"clf.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
